<!doctype html>
<html class="no-js" lang="en" data-content_root="">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<meta property="og:title" content="Golden rules for scaling" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://jeremybernste.in/modula/golden-rules/" />
<meta property="og:site_name" content="Modula" />
<meta property="og:description" content="So, you want to scale your training, huh? The good news first: it’s not too difficult. It boils down to a few simple principles and some basic linear algebra. The bad news? It requires unlearning a..." />
<meta property="og:image" content="https://jeremybernste.in/modula/_static/logo-square.jpeg" />
<meta property="og:image:alt" content="Modula" />
<meta name="description" content="So, you want to scale your training, huh? The good news first: it’s not too difficult. It boils down to a few simple principles and some basic linear algebra. The bad news? It requires unlearning a..." />
<link rel="index" title="Index" href="../genindex/" /><link rel="search" title="Search" href="../search/" /><link rel="next" title="The science of scale" href="../history/" /><link rel="prev" title="Bad scaling" href="../bad-scaling/" />

    <!-- Generated with Sphinx 7.0.0 and Furo 2024.05.06 -->
        <title>Golden rules for scaling - Modula documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?digest=7eec22261a5d8917218cec534344987d69e122f4" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" /
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../"><div class="brand">Modula  documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../_static/logo-light.svg" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../_static/logo-dark.svg" alt="Dark Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="../search/" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Introduction:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../bad-scaling/">Bad scaling</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Golden rules for scaling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../history/">The science of scale</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Theory of modules:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../theory/vector/">Vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../theory/module/">Modules</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../theory/atom/">Atomic modules</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Atomic modules</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../theory/atom/linear/">Linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="../theory/atom/embed/">Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../theory/atom/conv2d/">Conv2d</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../theory/bond/">Bond modules</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Bond modules</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../theory/bond/nonlinearities/">Nonlinearities</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../theory/compound/">Compound modules</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Compound modules</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../theory/compound/gpt/">GPT</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Useful links:</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/jxbz/modula">Modula codebase</a></li>
<li class="toctree-l1"><a class="reference external" href="https://arxiv.org/abs/2405.14813">Modula paper</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="https://github.com/jxbz/modula/blob/main/docs/source/golden-rules.rst?plain=true" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div><div class="edit-this-page">
  <a class="muted-link" href="https://github.com/jxbz/modula/edit/main/docs/source/golden-rules.rst" title="Edit this page">
    <svg><use href="#svg-pencil"></use></svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="golden-rules-for-scaling">
<h1>Golden rules for scaling<a class="headerlink" href="#golden-rules-for-scaling" title="Permalink to this heading">¶</a></h1>
<p>So, you want to scale your training, huh? The good news first: it’s not too difficult. It boils down to a few simple principles and some basic linear algebra. The bad news? It requires unlearning a few concepts you may have been taught in lectures. For example, consider the following principle:</p>
<blockquote>
<div><p>Initialize the weights so that all activations have unit variance at initialization.</p>
<p class="attribution">—Deep Learning 101</p>
</div></blockquote>
<p>This turns out to be bad for scaling. Why? Because the network internals can behave quite differently at initialization compared to after a few steps of training. A good way to understand this point is to consider a simple linear layer.</p>
<section id="the-linear-layer">
<h2>The linear layer<a class="headerlink" href="#the-linear-layer" title="Permalink to this heading">¶</a></h2>
<p>Consider a linear layer with Gaussian initialization and standard deviation <code class="docutils literal notranslate"><span class="pre">sigma</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Linear</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">fan_in</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">sigma</span><span class="p">:</span><span class="nb">float</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">fan_out</span><span class="p">,</span> <span class="n">fan_in</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The properties of this layer are most subtle when the layer conducts a large reduction in dimension—i.e. when <code class="docutils literal notranslate"><span class="pre">fan_out</span></code> is much smaller than <code class="docutils literal notranslate"><span class="pre">fan_in</span></code>. This might happen in the final layer of a classifier, for example. In fact, let’s study the case where we are scaling up <code class="docutils literal notranslate"><span class="pre">fan_in</span></code> while holding <code class="docutils literal notranslate"><span class="pre">fan_out</span></code> fixed.</p>
<p>An important fact about a matrix <code class="code highlight python docutils literal highlight-python"><span class="bp">self</span><span class="o">.</span><span class="n">weight</span></code> with <code class="docutils literal notranslate"><span class="pre">fan_in</span></code> much larger than <code class="docutils literal notranslate"><span class="pre">fan_out</span></code> is that the null space is huge, meaning that most of the input space is mapped to zero. The dimension of the null space is at least <code class="docutils literal notranslate"><span class="pre">fan_in</span> <span class="pre">-</span> <span class="pre">fan_out</span></code>. At initialization, most of a fixed input <code class="docutils literal notranslate"><span class="pre">x</span></code> will lie in this nullspace. This means that to get the output of <code class="code highlight python docutils literal highlight-python"><span class="bp">self</span><span class="o">.</span><span class="n">forward</span></code> to have unit variance at initialization, you need to pick a huge initialization scale <code class="docutils literal notranslate"><span class="pre">sigma</span></code> in order to scale up the component of <code class="docutils literal notranslate"><span class="pre">x</span></code> that does not lie in the null space. But after a few steps of training, the situation changes. Gradient descent will cause the input <code class="docutils literal notranslate"><span class="pre">x</span></code> to align with the non-null space of <code class="docutils literal notranslate"><span class="pre">self.weight</span></code>. This means that the <code class="docutils literal notranslate"><span class="pre">sigma</span></code> you chose to control the activations at initialization is now far too large in hindsight, and the activations will blow up! This problem only gets worse with increasing <code class="docutils literal notranslate"><span class="pre">fan_in</span></code>.</p>
<p>The solution to this problem is simple: don’t choose <code class="docutils literal notranslate"><span class="pre">sigma</span></code> to control variance at initialization! Instead, choose <code class="docutils literal notranslate"><span class="pre">sigma</span></code> under the assumption that inputs fall in the non-null space. Even if this makes the activations too small at initialization, this is fine as they will quickly “warm up” after a few steps of training. And for a nice bonus, we will show in the section on <a class="reference external" href="#fixing-width-scaling">width scaling</a> that switching from Gaussian init to orthogonal init makes choosing the right <code class="docutils literal notranslate"><span class="pre">sigma</span></code> trivial.</p>
</section>
<section id="three-golden-rules">
<h2>Three golden rules<a class="headerlink" href="#three-golden-rules" title="Permalink to this heading">¶</a></h2>
<p>The example in the previous section illustrates a style of thinking that extends far beyond linear layers. Let’s distill it into three key tenets, which we call the “golden rules” of scaling:</p>
<blockquote>
<div><ul class="starlist simple">
<li><p>Gradient descent causes inputs to align with the largest spectral components of tensors. So when initializing tensors, carefully set their largest spectral components.</p></li>
<li><p>The largest spectral components of gradient updates align with tensor inputs. So it is important to normalize gradient updates to control the size of their largest spectral components.</p></li>
<li><p>All layers will align during training. Keep this in mind when designing the architecture.</p></li>
</ul>
</div></blockquote>
<p>It’s worth expanding a little on what we mean by <em>alignment</em> here. When we say that an input <code class="docutils literal notranslate"><span class="pre">x</span></code> aligns with a weight matrix <code class="docutils literal notranslate"><span class="pre">weight</span></code>, we mean that if we compute <code class="docutils literal notranslate"><span class="pre">U,</span> <span class="pre">S,</span> <span class="pre">V</span> <span class="pre">=</span> <span class="pre">torch.linalg.svd(weight)</span></code>, then the input <code class="docutils literal notranslate"><span class="pre">x</span></code> will tend to have a larger dot product with the rows of <code class="docutils literal notranslate"><span class="pre">V</span></code> that correspond to larger diagonal entries of the singular value matrix <code class="docutils literal notranslate"><span class="pre">S</span></code>. When we say that layers align, we mean that the outputs of one layer will align with the next layer.</p>
<p>What’s the source of this alignment? Consider making a gradient update to a tensor in the middle of a deep net. We call all the preceding layers the “head” of the network, and all the layers after the “tail”:</p>
<figure class="align-default">
<img alt="../_images/alignment.svg" class="plot-directive" src="../_images/alignment.svg" /></figure>
<p>What’s important is that the gradient update “knows about” both the head of the network (through the layer inputs) and the tail of the network (through the backpropagated gradient). Applying the update will align the head with the tail <a class="footnote-reference brackets" href="#outerproduct" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. And this kind of alignment happens at all layers at every iteration!</p>
<p>The rest of this section will show how to apply the golden rules to do <a class="reference external" href="#fixing-width-scaling">width scaling</a>, <a class="reference external" href="#fixing-depth-scaling">depth scaling</a>, and <a class="reference external" href="#fixing-key-query-dot-product-scaling">key-query dot product scaling</a>. This should already be enough to get started scaling a GPT.</p>
</section>
<section id="fixing-width-scaling">
<h2>Fixing width scaling<a class="headerlink" href="#fixing-width-scaling" title="Permalink to this heading">¶</a></h2>
<p>First, let’s do width scaling in a linear layer. When the network has trained for a few steps to reach its fully aligned state, we want the input and output activations to fall roughly in the interval [-1, 1]. Equivalently, we want the inputs to have Euclidean length <code class="code highlight python docutils literal highlight-python"><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span></code> and the outputs to have Euclidean length <code class="code highlight python docutils literal highlight-python"><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_out</span><span class="p">)</span></code>. To achieve this, the first and second golden rules tell us that we need to control the top singular values of the initial weight matrix and the gradient updates. One can check that the right scaling is to set the singular values proportional to <code class="code highlight python docutils literal highlight-python"><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_out</span> <span class="o">/</span> <span class="n">fan_in</span><span class="p">)</span></code>. Intuitively, the factor of <code class="code highlight python docutils literal highlight-python"><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_out</span> <span class="o">/</span> <span class="n">fan_in</span><span class="p">)</span></code> means that the matrix operates as a “dimensional converter”: it takes in vectors of length <code class="code highlight python docutils literal highlight-python"><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span></code> and spits out vectors of length <code class="code highlight python docutils literal highlight-python"><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_out</span><span class="p">)</span></code>.</p>
<p>In fact we can be a little more clever here and reparameterize the linear layer as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ReparameterizedLinear</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">fan_in</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_out</span> <span class="o">/</span> <span class="n">fan_in</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">fan_out</span><span class="p">,</span> <span class="n">fan_in</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">orthogonal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>By including the conversion factor <code class="code highlight python docutils literal highlight-python"><span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_out</span> <span class="o">/</span> <span class="n">fan_in</span><span class="p">)</span></code> in the forward function, the correct scaling is to make the largest singular values of both <code class="code highlight python docutils literal highlight-python"><span class="bp">self</span><span class="o">.</span><span class="n">weight</span></code> and the weight updates order one. Easy, right? For the initialization, we can just use orthogonal init, which sets all the singular values to exactly one. In our experiments, we have found orthogonal init to be a performant, hyperparameter-free initializer. As for weight updates, we can just spectrally normalize them <a class="footnote-reference brackets" href="#spectralnorm" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span> <span class="o">/</span> <span class="n">spectral_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
<p>In practice, you may want to replace <code class="code highlight python docutils literal highlight-python"><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span></code> with some kind of momentum or Adam expression. And the learning rate can optionally decay through the course of training.</p>
</section>
<section id="fixing-depth-scaling">
<h2>Fixing depth scaling<a class="headerlink" href="#fixing-depth-scaling" title="Permalink to this heading">¶</a></h2>
<p>For depth scaling, we will look at scaling the number of blocks in a residual network <a class="footnote-reference brackets" href="#mlp" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> of the form:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">resnet</span><span class="p">(</span><span class="n">x</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">residue_list</span><span class="p">:</span><span class="nb">list</span><span class="p">,</span> <span class="n">block_multiplier</span><span class="p">:</span><span class="nb">float</span><span class="p">):</span>

    <span class="k">for</span> <span class="n">residue</span> <span class="ow">in</span> <span class="n">residue_list</span><span class="p">:</span>

        <span class="n">x</span> <span class="o">+=</span> <span class="n">block_multiplier</span> <span class="o">*</span> <span class="n">residue</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>We call this a residual network because at each iteration of the <code class="code highlight python docutils literal highlight-python"><span class="k">for</span></code> loop, a <code class="code highlight python docutils literal highlight-python"><span class="n">residue</span></code> is added to the input, which takes the form of a sub-network applied to the output from the previous step of the loop. The <code class="docutils literal notranslate"><span class="pre">block_multiplier</span></code> can be used to ensure that the residual contribution is small, allowing us to make the residual network very, very deep without its output blowing up. The main questions are:</p>
<ul class="simple">
<li><p>What kind of functions are we allowed to include in the <code class="docutils literal notranslate"><span class="pre">residue_list</span></code>?</p></li>
<li><p>What value should we choose for the <code class="docutils literal notranslate"><span class="pre">block_multiplier</span></code>?</p></li>
</ul>
<p>The third golden rule makes answering these questions easy. We should set <code class="code highlight python docutils literal highlight-python"><span class="n">block_multiplier</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">residue_list</span><span class="p">)</span></code>. This is because each residue adds one contribution to the output, and there are <code class="code highlight python docutils literal highlight-python"><span class="nb">len</span><span class="p">(</span><span class="n">residue_list</span><span class="p">)</span></code> residues in total. The sum of <code class="code highlight python docutils literal highlight-python"><span class="nb">len</span><span class="p">(</span><span class="n">residue_list</span><span class="p">)</span></code> aligned residues needs to be divided by <code class="code highlight python docutils literal highlight-python"><span class="nb">len</span><span class="p">(</span><span class="n">residue_list</span><span class="p">)</span></code> in order to not blow up. This is similar to an idea you may have seen in math that <span class="math notranslate nohighlight">\((1+\frac{1}{L})^L &lt; \mathrm{e}\)</span> for any <span class="math notranslate nohighlight">\(L&gt;0\)</span>. Even though the product may involve a large number <span class="math notranslate nohighlight">\(L\)</span> of terms, the residue <span class="math notranslate nohighlight">\(1/L\)</span> is small enough to prevent the product blowing up. Linking the analogy back to neural nets, <span class="math notranslate nohighlight">\(L\)</span> plays the role of <code class="code highlight python docutils literal highlight-python"><span class="nb">len</span><span class="p">(</span><span class="n">residue_list</span><span class="p">)</span></code>.</p>
<p>Since the <code class="code highlight python docutils literal highlight-python"><span class="mi">1</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">residue_list</span><span class="p">)</span></code> block multiplier prevents both the initialization and the updates to the residues from blowing up, we are safe to set each residue equal to any neural network of our choosing, so long as that network is individually initialized and updated in accordance with the golden rules <a class="footnote-reference brackets" href="#recursive" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>.</p>
</section>
<section id="fixing-key-query-dot-product-scaling">
<h2>Fixing key-query dot product scaling<a class="headerlink" href="#fixing-key-query-dot-product-scaling" title="Permalink to this heading">¶</a></h2>
<p>An important operation in transformers is taking the dot product between key and query vectors. Conventionally this is done as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">lambda</span> <span class="n">key</span><span class="p">,</span> <span class="n">query</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>The factor of <code class="code highlight python docutils literal highlight-python"><span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></code> is included to prevent the dot product from blowing up at initialization, where we assume that <code class="docutils literal notranslate"><span class="pre">key</span></code> and <code class="docutils literal notranslate"><span class="pre">query</span></code> are uncorrelated random vectors. But by the golden rules, we should expect that the keys and queries become aligned with each other through the course of training. Therefore we should instead normalize the dot product as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">lambda</span> <span class="n">key</span><span class="p">,</span> <span class="n">query</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span> <span class="o">/</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>To spell this out more clearly, the dot product is the sum of a number <code class="code highlight python docutils literal highlight-python"><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code> of aligned quantities, so we should divide by <code class="code highlight python docutils literal highlight-python"><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code> to prevent the sum blowing up.</p>
</section>
<section id="wrapping-up">
<h2>Wrapping up<a class="headerlink" href="#wrapping-up" title="Permalink to this heading">¶</a></h2>
<p>On this page, we introduced three “golden rules” for scaling and pointed out how they differ to some conventional wisdom about controlling activation variance at initialization. One of the points we hope to get across is that the logical reasoning associated with the golden rules is not only <em>more scalable</em> but also <em>simpler</em> than standard approaches based on controlling variance. You don’t need to know anything about how random variables behave in order to get scaling right—you just need to know how objects add when they point in the same direction. Furthermore, the use of orthogonal initialization obviates the need to know anything about the spectral properties of Gaussian random matrices.</p>
<p>In the next section we will look at the history behind these ideas, and after that we will explain how Modula automates the application of the golden rules.</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="outerproduct" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>The mathematical analogue of this intuitive statement is to say that the gradient of a linear layer is an outer product of the layer input with the gradient of the loss with respect to the layer output.</p>
</aside>
<aside class="footnote brackets" id="spectralnorm" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">2</a><span class="fn-bracket">]</span></span>
<p>The spectral norm of a matrix is the largest singular value. The largest singular value of <code class="code highlight python docutils literal highlight-python"><span class="n">matrix</span> <span class="o">/</span> <span class="n">spectral_norm</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span></code> is always one, so long as <code class="code highlight python docutils literal highlight-python"><span class="n">matrix</span> <span class="o">!=</span> <span class="mi">0</span></code>.</p>
</aside>
<aside class="footnote brackets" id="mlp" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">3</a><span class="fn-bracket">]</span></span>
<p>We study residual networks over MLPs because MLPs seem to just work bad beyond depth 10 or so. In the Modula paper, we show that the type of residual networks we propose are in fact “smooth” even in the limit of infinitely many blocks. The same property does not hold for MLPs to the best of our knowledge.</p>
</aside>
<aside class="footnote brackets" id="recursive" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">4</a><span class="fn-bracket">]</span></span>
<p>The recursive nature of this statement directly inspired the Modula framework.</p>
</aside>
</aside>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../history/">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">The science of scale</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="../bad-scaling/">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Bad scaling</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024, modula-authors
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/jxbz/modula" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Golden rules for scaling</a><ul>
<li><a class="reference internal" href="#the-linear-layer">The linear layer</a></li>
<li><a class="reference internal" href="#three-golden-rules">Three golden rules</a></li>
<li><a class="reference internal" href="#fixing-width-scaling">Fixing width scaling</a></li>
<li><a class="reference internal" href="#fixing-depth-scaling">Fixing depth scaling</a></li>
<li><a class="reference internal" href="#fixing-key-query-dot-product-scaling">Fixing key-query dot product scaling</a></li>
<li><a class="reference internal" href="#wrapping-up">Wrapping up</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/scripts/furo.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/tabs.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>