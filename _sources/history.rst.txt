The science of scale
=====================

.. admonition:: Warning
   :class: warning

   This page is still under construction.

.. admonition:: Warning
   :class: seealso

   This page was written by Jeremy and so is likely biased by his view of the world. He is putting this here because he thinks it provides a useful counterpoint to some prevailing narratives. If you'd like to mention some other work here, feel free to either make a pull request or reach out to us by email.

some twists and turns

   | ðŸ“˜ `On the distance between two neural networks and the stability of learning <https://arxiv.org/abs/2002.03432>`_
   |     Jeremy Bernstein, Arash Vahdat, Yisong Yue, Ming-Yu Liu
   |     NeurIPS 2020

some more text

   | ðŸ“™ `Feature learning in infinite-width neural networks <https://arxiv.org/abs/2011.14522>`_
   |     Greg Yang, Edward J. Hu
   |     ICML 2021

and more text

   | ðŸ“— `A spectral condition for feature learning <https://arxiv.org/abs/2310.17813>`_
   |     Greg Yang, James B. Simon, Jeremy Bernstein
   |     arXiv 2023

and more

   | ðŸ“’ `Automatic gradient descent: Deep learning without hyperparameters <https://arxiv.org/abs/2304.05187>`_
   |     Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan, Yisong Yue
   |     arXiv 2023

and even more text

   | ðŸ“• `Scalable optimization in the modular norm <https://arxiv.org/abs/2405.14813>`_
   |     Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, Jeremy Bernstein
   |     arXiv 2024